{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Reference :\n",
    "http://cs231n.github.io/neural-networks-2/  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "img_size = 784\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urllib.request.urlretrieve(source + filename, filename)\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        \n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        \n",
    "        data = data.reshape(-1, img_size)\n",
    "        \n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        \n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    # change labels into one hot vectors\n",
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "        \n",
    "        return T\n",
    "\n",
    "    \n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "    \n",
    "    y_train = _change_one_hot_label(y_train)\n",
    "    y_test =  _change_one_hot_label(y_test)\n",
    "\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "X = np.concatenate([X_train, X_val], axis=0)\n",
    "y = np.concatenate([y_train, y_val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "    \n",
    "    x = x - np.max(x) \n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:   \n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "\n",
    "class Three_Layer_NN_dropout:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_1_size, hidden_2_size, output_size, std=0.01):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_1_size)\n",
    "        self.params['b1'] = np.zeros(hidden_1_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_1_size, hidden_2_size)\n",
    "        self.params['b2'] = np.zeros(hidden_2_size)\n",
    "        self.params['W3'] = std * np.random.randn(hidden_2_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "    \n",
    "    \n",
    "    def loss(self, X, t=None):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "        \n",
    "        p = 0.5\n",
    "        \n",
    "        # forward\n",
    "        H1 = np.dot(X, W1) + b1    \n",
    "        H1 = np.maximum(0, H1)    # ReLu\n",
    "        U1 = (np.random.rand(*H1.shape) < p) / p    \n",
    "        H1 *= U1                                    \n",
    "        \n",
    "        H2 = np.dot(H1, W2) + b2\n",
    "        H2 = np.maximum(0, H2)    # ReLu\n",
    "        U2 = (np.random.rand(*H2.shape) < p) / p\n",
    "        H2 *= U2                                    \n",
    "        \n",
    "        H3 = np.dot(H2, W3) + b3\n",
    "        y = softmax(H3)     \n",
    "                \n",
    "        if t is None:\n",
    "            return y\n",
    "        \n",
    "        # loss function\n",
    "        loss = cross_entropy_error(y, t)    \n",
    "        \n",
    "        # back propagation\n",
    "        grads = {}\n",
    "        batch_num = X.shape[0]\n",
    "        \n",
    "        delta_3 = (y - t) / batch_num        \n",
    "        grads['W3'] = np.dot(H2.T, delta_3)\n",
    "        grads['b3'] = np.sum(delta_3, axis=0)\n",
    "        \n",
    "        relu_diff_2 = (H2 > 0) * np.ones([*H2.shape])\n",
    "        delta_2 = (np.dot(delta_3, W3.T)) * relu_diff_2\n",
    "        grads['W2'] = np.dot(H1.T, delta_2)\n",
    "        grads['b2'] = np.sum(delta_2, axis=0)\n",
    "        \n",
    "        relu_diff_1 = (H1 > 0) * np.ones([*H1.shape])\n",
    "        delta_1 = (np.dot(delta_2, W2.T)) * relu_diff_1\n",
    "        grads['W1'] = np.dot(X.T, delta_1)\n",
    "        grads['b1'] = np.sum(delta_1, axis=0)\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    # Training by SGD with dropout\n",
    "    def train(self, X, t, X_val, y_val, learning_rate=0.1, num_iters=10000, batch_size=100):\n",
    "        \n",
    "        train_acc_list = []\n",
    "        val_acc_list = []\n",
    "        \n",
    "        iter_per_epoch = 500\n",
    "        \n",
    "        for i in range(num_iters):\n",
    "            batch = np.random.choice(X.shape[0], batch_size)\n",
    "            X_batch = X[batch]\n",
    "            t_batch = t[batch]\n",
    "            \n",
    "            loss, grads = self.loss(X_batch, t_batch)\n",
    "            \n",
    "            for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "                self.params[key] -= learning_rate * grads[key] \n",
    "            \n",
    "            if i % iter_per_epoch == 0:\n",
    "                train_acc = self.accuracy(X, t)\n",
    "                val_acc = self.accuracy(X_val, y_val)\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "                print(i, \": train acc, val acc | \" + str(train_acc) + \", \" + str(val_acc))\n",
    "                \n",
    "        print(\"Finish!\")\n",
    "        return train_acc_list, val_acc_list\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "        \n",
    "        H1 = np.dot(X, W1) + b1    \n",
    "        H1 = np.maximum(0, H1)    # ReLu\n",
    "        \n",
    "        H2 = np.dot(H1, W2) + b2\n",
    "        H2 = np.maximum(0, H2)    # ReLu\n",
    "        \n",
    "        H3 = np.dot(H2, W3) + b3\n",
    "        y = softmax(H3)     \n",
    "\n",
    "  \n",
    "        return y\n",
    "    \n",
    "    def accuracy(self, X, t):\n",
    "        y = self.predict(X)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(X.shape[0])\n",
    "        return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : train acc, val acc | 0.09718, 0.0983\n",
      "500 : train acc, val acc | 0.91838, 0.9245\n",
      "1000 : train acc, val acc | 0.95238, 0.9543\n",
      "1500 : train acc, val acc | 0.9655, 0.9644\n",
      "2000 : train acc, val acc | 0.97116, 0.9668\n",
      "2500 : train acc, val acc | 0.9775, 0.9717\n",
      "3000 : train acc, val acc | 0.98032, 0.9728\n",
      "3500 : train acc, val acc | 0.98312, 0.9759\n",
      "4000 : train acc, val acc | 0.98558, 0.9782\n",
      "4500 : train acc, val acc | 0.9868, 0.9786\n",
      "5000 : train acc, val acc | 0.98868, 0.9781\n",
      "5500 : train acc, val acc | 0.98908, 0.9787\n",
      "6000 : train acc, val acc | 0.99098, 0.9801\n",
      "6500 : train acc, val acc | 0.99172, 0.9798\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "network = Three_Layer_NN_dropout(input_size=784, hidden_1_size=500, hidden_2_size=255, \n",
    "                         output_size=10, std=0.01)\n",
    "\n",
    "_, _ = network.train(X_train, y_train, X_val, y_val, learning_rate=0.4, num_iters=7000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98089999999999999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train by (train + val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : train acc, val acc | 0.112366666667, 0.1064\n",
      "500 : train acc, val acc | 0.9014, 0.9108\n",
      "1000 : train acc, val acc | 0.953166666667, 0.9584\n",
      "1500 : train acc, val acc | 0.963783333333, 0.9689\n",
      "2000 : train acc, val acc | 0.965483333333, 0.97\n",
      "2500 : train acc, val acc | 0.973016666667, 0.9758\n",
      "3000 : train acc, val acc | 0.97905, 0.9786\n",
      "3500 : train acc, val acc | 0.9821, 0.9826\n",
      "4000 : train acc, val acc | 0.983766666667, 0.9853\n",
      "4500 : train acc, val acc | 0.98515, 0.9855\n",
      "5000 : train acc, val acc | 0.986133333333, 0.9868\n",
      "5500 : train acc, val acc | 0.987483333333, 0.989\n",
      "6000 : train acc, val acc | 0.988883333333, 0.9888\n",
      "6500 : train acc, val acc | 0.99, 0.9909\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "network = Three_Layer_NN_dropout(input_size=784, hidden_1_size=500, hidden_2_size=255, \n",
    "                         output_size=10, std=0.01)\n",
    "\n",
    "_, _ = network.train(X, y, X_val, y_val, learning_rate=0.4, num_iters=7000, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98140000000000005"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
